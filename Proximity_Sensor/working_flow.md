# **Complete Workflow Documentation**

## **ğŸ“– Project Workflow in Detail**

### **1. Overall System Flow**

The proximity sensor monitoring system follows a **three-tier architecture** where each component handles specific responsibilities:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TIER 1    â”‚   â”‚   TIER 2    â”‚   â”‚   TIER 3    â”‚
â”‚  Data       â”‚   â”‚  Processing â”‚   â”‚  Presentationâ”‚
â”‚  Acquisitionâ”‚   â”‚  & Storage  â”‚   â”‚  & Control  â”‚
â”‚             â”‚   â”‚             â”‚   â”‚             â”‚
â”‚  C Client   â”‚â”€â”€â–¶â”‚  C++ Server â”‚â”€â”€â–¶â”‚  Python     â”‚
â”‚  (Sensors)  â”‚   â”‚  (Middleware)â”‚   â”‚  Dashboard  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                  â”‚                  â”‚
  Physical        Data Processing    User Interaction
  Hardware            Logic           & Visualization
```

### **2. Step-by-Step Operational Workflow**

#### **Phase 1: System Initialization**

```
Step 1.1: Server Boot
â”œâ”€â”€ C++ server starts and initializes:
â”‚   â”œâ”€â”€ Creates TCP socket on port 5000
â”‚   â”œâ”€â”€ Creates WebSocket server on port 8080
â”‚   â”œâ”€â”€ Initializes SQLite database/CSV file
â”‚   â”œâ”€â”€ Loads configuration from config.json
â”‚   â””â”€â”€ Starts listening for connections

Step 1.2: Client Initialization
â”œâ”€â”€ C client program starts:
â”‚   â”œâ”€â”€ Reads sensor configuration
â”‚   â”œâ”€â”€ Initializes GPIO pins (for real hardware)
â”‚   â”œâ”€â”€ Establishes connection with server
â”‚   â””â”€â”€ Enters main sensing loop

Step 1.3: Dashboard Launch
â”œâ”€â”€ Python dashboard starts:
â”‚   â”œâ”€â”€ Loads GUI components (Tkinter)
â”‚   â”œâ”€â”€ Connects to WebSocket server (port 8080)
â”‚   â”œâ”€â”€ Loads historical data from CSV
â”‚   â””â”€â”€ Displays real-time interface
```

#### **Phase 2: Continuous Data Flow**

```
Step 2.1: Sensor Reading (Every 2 seconds)
â”œâ”€â”€ C Client (sensor_client.c):
â”‚   â”œâ”€â”€ Calls read_sensor_distance():
â”‚   â”‚   â”œâ”€â”€ (Simulated) Generates random distance 0-100cm
â”‚   â”‚   â”œâ”€â”€ (Real) Triggers ultrasonic sensor & measures echo
â”‚   â”‚   â””â”€â”€ Applies noise and object movement simulation
â”‚   â”œâ”€â”€ Creates JSON payload:
â”‚   â”‚   {
â”‚   â”‚     "sensor_id": "sensor_01",
â”‚   â”‚     "distance": 45.5,
â”‚   â”‚     "timestamp": "2024-01-15 14:30:25"
â”‚   â”‚   }
â”‚   â””â”€â”€ Sends via TCP to server:5000

Step 2.2: Server Processing
â”œâ”€â”€ C++ Server (server.cpp):
â”‚   â”œâ”€â”€ TCP Receiver thread accepts data
â”‚   â”œâ”€â”€ Parses JSON using nlohmann/json library
â”‚   â”œâ”€â”€ Calculates status based on thresholds:
â”‚   â”‚   â”œâ”€â”€ < 10cm â†’ CRITICAL (Red/Purple)
â”‚   â”‚   â”œâ”€â”€ 10-30cm â†’ WARNING (Orange)
â”‚   â”‚   â”œâ”€â”€ 30-50cm â†’ CAUTION (Yellow)
â”‚   â”‚   â””â”€â”€ > 50cm â†’ SAFE (Green)
â”‚   â”œâ”€â”€ Stores in CSV database:
â”‚   â”‚   timestamp,sensor_id,distance,status
â”‚   â””â”€â”€ Broadcasts via WebSocket to all connected dashboards

Step 2.3: Dashboard Update
â”œâ”€â”€ Python Dashboard (dashboard.py):
â”‚   â”œâ”€â”€ WebSocket client receives JSON data
â”‚   â”œâ”€â”€ Updates sensor widgets in real-time:
â”‚   â”‚   â”œâ”€â”€ Distance display with colored text
â”‚   â”‚   â”œâ”€â”€ Progress bar showing proximity level
â”‚   â”‚   â”œâ”€â”€ Status indicator with color coding
â”‚   â”‚   â””â”€â”€ Last update timestamp
â”‚   â”œâ”€â”€ Updates historical chart:
â”‚   â”‚   â”œâ”€â”€ Adds new data point to time-series
â”‚   â”‚   â”œâ”€â”€ Maintains last 50 readings per sensor
â”‚   â”‚   â””â”€â”€ Animates chart with smooth transitions
â”‚   â””â”€â”€ Logs events to text window
```

#### **Phase 3: User Interaction Flow**

```
Step 3.1: Monitoring
â”œâ”€â”€ User observes:
â”‚   â”œâ”€â”€ Real-time distance readings from multiple sensors
â”‚   â”œâ”€â”€ Color-coded status alerts
â”‚   â”œâ”€â”€ Historical trends on chart
â”‚   â””â”€â”€ Event log for system activities

Step 3.2: Control Actions
â”œâ”€â”€ User can:
â”‚   â”œâ”€â”€ Export data to CSV with timestamp
â”‚   â”œâ”€â”€ Adjust chart history length
â”‚   â”œâ”€â”€ Change WebSocket server address
â”‚   â””â”€â”€ Refresh connections manually

Step 3.3: Alert Response
â”œâ”€â”€ System triggers:
â”‚   â”œâ”€â”€ Visual alerts on dashboard
â”‚   â”œâ”€â”€ Color changes (Greenâ†’Yellowâ†’Redâ†’Purple)
â”‚   â”œâ”€â”€ Progress bar updates
â”‚   â””â”€â”€ Log entries for critical events
```

#### **Phase 4: Error Handling & Recovery**

```
Step 4.1: Connection Issues
â”œâ”€â”€ If client loses connection:
â”‚   â”œâ”€â”€ Attempts automatic reconnection every 5 seconds
â”‚   â”œâ”€â”€ Buffers sensor readings locally (future enhancement)
â”‚   â””â”€â”€ Logs connection attempts

Step 4.2: Data Corruption
â”œâ”€â”€ If invalid data received:
â”‚   â”œâ”€â”€ Server sends NACK (negative acknowledgment)
â”‚   â”œâ”€â”€ Client retransmits after delay
â”‚   â””â”€â”€ Error logged to server.log

Step 4.3: Dashboard Disconnect
â”œâ”€â”€ If dashboard loses WebSocket connection:
â”‚   â”œâ”€â”€ Shows "Disconnected" status
â”‚   â”œâ”€â”€ Attempts auto-reconnect
â”‚   â””â”€â”€ Resumes normal operation on reconnection
```

### **3. Docker Implementation Workflow**

#### **Why Docker for This Project?**

Docker provides:
1. **Isolation** - Each component runs in its own container
2. **Portability** - Runs consistently across different systems
3. **Easy Deployment** - One command to start entire system
4. **Resource Management** - Controlled CPU/Memory allocation
5. **Network Isolation** - Secure internal communication

#### **Docker Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Docker Host System                 â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚â”‚
â”‚  â”‚   C++      â”‚  â”‚   C        â”‚  â”‚   Python   â”‚â”‚
â”‚  â”‚  Server    â”‚  â”‚  Client    â”‚  â”‚  Dashboard â”‚â”‚
â”‚  â”‚  Container â”‚  â”‚  Container â”‚  â”‚  Container â”‚â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚        â”‚                â”‚                â”‚      â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                         â”‚                       â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                 â”‚  Docker       â”‚               â”‚
â”‚                 â”‚  Network      â”‚               â”‚
â”‚                 â”‚  (proximity-net)              â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Docker Workflow Steps**

##### **Step 1: Build Docker Images**

```bash
# Build all three images from Dockerfiles
docker-compose build

# This executes:
# 1. Builds C++ server image (FROM gcc:11)
# 2. Builds C client image (FROM gcc:11)
# 3. Builds Python dashboard image (FROM python:3.9-slim)
```

##### **Step 2: Container Network Setup**

```yaml
# docker-compose.yml creates:
# - Internal Docker network 'proximity-net'
# - Port mappings:
#   Server: 5000 (TCP) and 8080 (WebSocket) exposed
#   Dashboard: 8081 (GUI) exposed
#   Client: No ports exposed (internal only)
```

##### **Step 3: Container Startup Sequence**

```
Time 0s: Docker Compose starts network
Time 1s: Server container starts (depends on nothing)
â”œâ”€â”€ Runs ./proximity_server
â”œâ”€â”€ Opens ports 5000 and 8080
â”œâ”€â”€ Waits for connections
â””â”€â”€ Logs: "Server listening on 0.0.0.0:5000"

Time 5s: Client container starts (depends_on: server)
â”œâ”€â”€ Runs sensor_client
â”œâ”€â”€ Connects to server:5000
â”œâ”€â”€ Starts sensor simulation
â””â”€â”€ Logs: "Sensor sensor_01 started"

Time 7s: Dashboard container starts (depends_on: server)
â”œâ”€â”€ Runs python dashboard.py
â”œâ”€â”€ Connects to ws://server:8080/sensors
â”œâ”€â”€ Launches Tkinter GUI
â””â”€â”€ Logs: "Dashboard started successfully"
```

##### **Step 4: Inter-Container Communication**

```
# Inside Docker network:
Client â†’ Server: Uses internal DNS 'proximity-server:5000'
Dashboard â†’ Server: Uses 'ws://proximity-server:8080/sensors'

# External access (from host machine):
Web browser: http://localhost:8081 (if web dashboard)
TCP test: nc localhost 5000
WebSocket test: wscat -c ws://localhost:8080/sensors
```

##### **Step 5: Data Persistence**

```yaml
# Volumes configuration:
volumes:
  - ./data:/app/data    # Shared CSV database
  - ./logs:/app/logs    # Shared log files
  
# Each container sees the same /app/data directory
# Data persists even after containers are stopped
```

##### **Step 6: Monitoring & Management**

```bash
# View all running containers
docker-compose ps

# View logs (all containers)
docker-compose logs -f

# View specific container logs
docker-compose logs -f proximity-server

# Access container shell
docker-compose exec proximity-server /bin/bash

# Check resource usage
docker stats

# Scale sensor clients (if needed)
docker-compose up --scale sensor-client=5
```

#### **Docker Development Workflow**

##### **For Development:**

```bash
# 1. Start with bind mounts for live code updates
docker-compose -f docker-compose.dev.yml up

# 2. This mounts local source code into containers:
#    - ./server.cpp â†’ /app/server.cpp
#    - ./sensor_client.c â†’ /app/sensor_client.c
#    - ./dashboard.py â†’ /app/dashboard.py

# 3. Changes to local files instantly reflect in containers
# 4. Use volume for dependencies to avoid re-downloading
```

##### **For Production:**

```bash
# 1. Build optimized images
docker-compose -f docker-compose.prod.yml build

# 2. Push to container registry
docker-compose push

# 3. Deploy to production server
docker-compose -f docker-compose.prod.yml up -d

# 4. Set up monitoring
docker-compose logs --tail=100 -f
```

#### **Dockerfile Breakdown**

##### **C++ Server Dockerfile Strategy:**

```dockerfile
# Multi-stage build:
# Stage 1: Builder (includes compilers and build tools)
FROM gcc:11 as builder
# Install build dependencies
# Compile server binary

# Stage 2: Runtime (minimal base image)
FROM ubuntu:22.04
# Copy only compiled binary
# Install runtime dependencies only
# Result: Small, secure image
```

##### **C Client Dockerfile Strategy:**

```dockerfile
# Single-stage build (simpler)
FROM gcc:11
# Install dependencies
# Compile binary
# Set entrypoint script for configuration
```

##### **Python Dashboard Dockerfile Strategy:**

```dockerfile
# Optimized Python image
FROM python:3.9-slim
# Install system dependencies for GUI
# Copy requirements.txt first (caching optimization)
# Install Python packages
# Copy application code
```

#### **Docker Compose Configuration Details**

```yaml
services:
  proximity-server:
    build:
      context: .  # Uses current directory
      dockerfile: Dockerfile.server
    container_name: proximity-server
    ports:
      - "5000:5000"  # Host:Container port mapping
      - "8080:8080"
    volumes:
      - ./data:/app/data  # Bind mount for data persistence
      - ./logs:/app/logs
    networks:
      - proximity-net  # Custom network for isolation
    environment:
      - LOG_LEVEL=INFO  # Environment variables
    restart: unless-stopped  # Auto-restart on failure
    healthcheck:  # Container health monitoring
      test: ["CMD", "nc", "-z", "localhost", "5000"]
      interval: 30s
      timeout: 10s
      retries: 3
```

#### **Advanced Docker Features Used**

##### **1. Health Checks:**
```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

##### **2. Resource Limits:**
```yaml
deploy:
  resources:
    limits:
      cpus: '0.50'
      memory: 512M
    reservations:
      cpus: '0.25'
      memory: 256M
```

##### **3. Secrets Management:**
```yaml
secrets:
  db_password:
    file: ./secrets/db_password.txt
```

##### **4. Logging Configuration:**
```yaml
logging:
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
```

#### **Docker Deployment Scenarios**

##### **Scenario 1: Local Development**
```bash
# Using development compose file with hot reload
docker-compose -f docker-compose.dev.yml up

# Features:
# - Source code mounted for live editing
# - Debug tools included
# - Volume for dependency caching
```

##### **Scenario 2: Testing/CI**
```bash
# Using test compose file
docker-compose -f docker-compose.test.yml up

# Features:
# - Test data pre-loaded
# - Mock sensors for testing
# - Automated test execution
```

##### **Scenario 3: Production Deployment**
```bash
# Using production compose file
docker-compose -f docker-compose.prod.yml up -d

# Features:
# - Optimized images
# - Resource limits
# - Health checks
# - Log aggregation
# - Backup volumes
```

#### **Docker Commands Cheat Sheet**

```bash
# Build and start all services
docker-compose up --build

# Start in background
docker-compose up -d

# Stop all services
docker-compose down

# View logs
docker-compose logs -f

# Execute command in container
docker-compose exec proximity-server ls -la

# Scale services
docker-compose up --scale sensor-client=3

# View resource usage
docker-compose top

# Copy files from container
docker cp proximity-server:/app/logs/server.log ./local/

# Backup volumes
docker run --rm -v proximity-system_data:/data -v $(pwd):/backup \
  ubuntu tar czf /backup/backup.tar.gz /data
```

#### **Docker Security Best Practices Applied**

1. **Non-root users** in containers
2. **Read-only root filesystem** where possible
3. **Resource limits** to prevent DoS
4. **Network segmentation** with custom networks
5. **Secret management** for credentials
6. **Regular updates** of base images
7. **Image scanning** for vulnerabilities

### **4. Real-World Deployment Scenarios**

#### **Scenario A: Single Board Computer (Raspberry Pi)**
```
Physical Setup:
â”œâ”€â”€ Raspberry Pi 4 (4GB RAM)
â”œâ”€â”€ HC-SR04 Ultrasonic Sensors (connected via GPIO)
â”œâ”€â”€ Running all three components natively
â””â”€â”€ Local display for dashboard

Docker Advantage:
â”œâ”€â”€ Easy updates without breaking dependencies
â”œâ”€â”€ Isolated sensor access
â””â”€â”€ Backup/restore capability
```

#### **Scenario B: Industrial Monitoring**
```
Physical Setup:
â”œâ”€â”€ Multiple sensor nodes (C clients) across factory
â”œâ”€â”€ Central server in control room
â”œâ”€â”€ Multiple dashboard stations
â””â”€â”€ Historical data analysis server

Docker Advantage:
â”œâ”€â”€ Scalable with container orchestration (Kubernetes)
â”œâ”€â”€ Centralized logging with ELK stack
â”œâ”€â”€ High availability with container replication
â””â”€â”€ Rolling updates without downtime
```

#### **Scenario C: Cloud Deployment**
```
Physical Setup:
â”œâ”€â”€ Sensors with cellular/WiFi connectivity
â”œâ”€â”€ Cloud server (AWS/Azure/Google Cloud)
â”œâ”€â”€ Web-based dashboard accessible globally
â””â”€â”€ Mobile app for alerts

Docker Advantage:
â”œâ”€â”€ Container registry for easy deployment
â”œâ”€â”€ Auto-scaling based on sensor count
â”œâ”€â”€ Cloud-native monitoring integration
â””â”€â”€ Cost optimization with resource limits
```

### **5. Performance Optimization**

#### **Memory Optimization:**
```c
// C Client: Efficient memory usage
- Stack allocation for sensor data
- Reusable JSON buffers
- Minimal library dependencies

// C++ Server: Connection pooling
- Thread pool for client handling
- Memory pool for JSON objects
- Connection reuse

// Python Dashboard: Lazy loading
- Chart updates only when data changes
- Widget creation on-demand
- Garbage collection optimization
```

#### **Network Optimization:**
```
1. TCP Nagle's algorithm disabled for low latency
2. WebSocket compression enabled
3. Binary protocol option for high-frequency sensors
4. Connection keep-alive to reduce handshake overhead
```

### **6. Extension Points**

#### **Future Enhancements:**

1. **MQTT Support** - Add MQTT broker for IoT integration
2. **Database Backend** - Replace CSV with PostgreSQL/InfluxDB
3. **Mobile App** - React Native app for mobile monitoring
4. **Alert System** - Email/SMS notifications for critical events
5. **Machine Learning** - Predictive maintenance using historical data
6. **Edge Computing** - Local processing on sensor nodes
7. **Blockchain** - Immutable audit trail for sensor data

### **7. Maintenance Workflow**

#### **Daily Operations:**
```
1. Check system status: make status
2. Review logs: tail -f logs/server.log
3. Backup data: ./scripts/backup.sh
4. Monitor resource usage: docker stats
5. Check for updates: ./scripts/update-check.sh
```

#### **Weekly Operations:**
```
1. Rotate logs: logrotate configuration
2. Clean old data: ./scripts/cleanup.sh
3. Update containers: docker-compose pull
4. Security scan: trivy image proximity-server
5. Performance review: analyze logs/performance.log
```

### **8. Troubleshooting Flowchart**

```
Start: Issue detected
  â”‚
  â–¼
Is server running? â†’ No â†’ Start server: ./proximity_server
  â”‚ Yes
  â–¼
Can client connect? â†’ No â†’ Check network/firewall
  â”‚ Yes
  â–¼
Is data being received? â†’ No â†’ Check sensor hardware
  â”‚ Yes
  â–¼
Is dashboard showing data? â†’ No â†’ Check WebSocket connection
  â”‚ Yes
  â–¼
System operational âœ“
```

This comprehensive workflow ensures that the proximity sensor monitoring system operates reliably, efficiently, and can be easily deployed and maintained using modern containerization techniques.
